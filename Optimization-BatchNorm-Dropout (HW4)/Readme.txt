In this homework, we have implemented variations of gradient descenet algorithm including Momentum, Nesterov Nomentum, Adagrad, RMSProp, and Adam. Further, we implemented batch norm algorithm which would "whiten" the neurons statistics before ReLU in each hidden layer. This will also make the net to be less sensitive to initial values of the weights. Lastly, we implemented the (inverted) dropout algorithm as a mean to regularize the net.    
