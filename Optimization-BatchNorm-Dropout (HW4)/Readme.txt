In this homework, we have implemented variations of gradient descenet algorithm including momentum, RMSProp, and Adam. Further, we implemented batch norm algorithm which would "whiten" the neurons statistics before ReLU. This will also make the net to be less sensitive to initial values of the weights. Lastly, we implemented the (inverted) dropout algorithm as a mean to regularize the net.    
