In this homework, we first implement a two-layer neural network with ReLU activation function and Softmax classification (two_layer_nn.ipynob and neural_net.py). After, we modularize the code so that the net can have any arbitrary numbers of hidden layers and neurons, any non-linear activation function, as well as either SVM or Softmax for the classification (FC_nets.ipynb, fc_net.py, layers.py, layer_utils.py).
